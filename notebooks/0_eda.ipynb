{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e56357",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "This notebook employs standard data science and ML libraries (`numpy`, `pandas`, `scikit-learn`) and visualization libraries (`matplotlib` and `seaborn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# No warnings about setting value on copy of slice\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Display up to 60 columns of a dataframe\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "# Matplotlib visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Internal ipython tool for setting figure size\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Seaborn for visualization\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# Splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "# plot formatting\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f832e",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "We'll load the raw csv file into a pandas dataframe for exploration. Based on the metadata, we know that the column 'Property Id' is unique for each row and so we set that as the index column. We'll start exploring some basic features about dataset by inspecting the first few rows, inspecting the shape of the dataset, and sifting through columns that are already set with a numerical data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3460fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data file\n",
    "#data = pd.read_csv(\"../data/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2017__Data_for_Calendar_Year_2016_.csv\", index_col=\"Property Id\")\n",
    "data = pd.read_csv(\"../data/NYC_Building_Energy_and_Water_Data_Disclosure_for_Local_Law_84__2022-Present__20250609.csv\", index_col=\"Property ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few lines\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d17fb9",
   "metadata": {},
   "source": [
    "Next we'll print some basic information about the dataset: its shape, features, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape information \n",
    "print(f\"Shape of dataset: {data.shape}\")\n",
    "\n",
    "# Numeric features\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "print(\"\\nNumeric Columns:\",  numeric_cols)\n",
    "\n",
    "# Numeric features\n",
    "nonnumeric_cols = data.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nNon-numeric Columns:\",  nonnumeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2067d",
   "metadata": {},
   "source": [
    "# Clean numeric columns\n",
    "\n",
    "Looking at the cell above, we see that there are many columns which hold numerical data, such as those in units of ftÂ² and kBtu. Based on the `data.head()` information, we also know that missing data are indicated with cells that store the string `\"Not Available.\"`. Let's start the data cleaning process by first changing all missing values to `nan`. Next we'll identify columns that store numerical data by checking if they contain a unit (energy, volume, mass, etc.) and convert them to `float` types. Then we can view summary stats for all numeric columns with `data.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ac46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all instances of 'Not Available' with nan\n",
    "data = data.replace({'Not Available': np.nan})\n",
    "\n",
    "# We'll also manually add the ENERGY STAR Score column\n",
    "\n",
    "units = ['ft', 'kBtu', 'Number of', 'Percent', 'Hours', 'gal', 'Tons', 'kWh', 'ENERGY STAR Score', 'kgal']\n",
    "\n",
    "for col in data.columns:\n",
    "    # check if there are any units or any other indication that this column is numeric\n",
    "    for token in units:\n",
    "        if token in col:\n",
    "            try:\n",
    "                data[col] = data[col].astype(float)\n",
    "            except:\n",
    "                print(f\"Couldn't convert column '{col}'.\")\n",
    "\n",
    "# summary stats on numeric columns\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da473e21",
   "metadata": {},
   "source": [
    "The definition for columns regarding 'metered areas' is a bit unclear, so we can inspect those columns by checking what unique values they store. Based on the results, a reasonable assumption is that metered areas refers to areas of the building for which energy and water usage data is measurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique values for 'Metered Areas (Water)':\\n\", data['Metered Areas (Water)'].unique())\n",
    "\n",
    "print(\"\\nUnique values for 'Metered Areas (Energy)':\\n\", data['Metered Areas (Energy)'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce9285",
   "metadata": {},
   "source": [
    "We want to predict the \"ENERGY STAR Score\" column for unseen data points (NYC properties). Looking at the cells above, we see that this column is classified as non-numeric. To investigate, clean, and convert this column, let's print the first few values.\n",
    "\n",
    "The official definition of the target column is as follows, directly taken from this [link](https://data.cityofnewyork.us/Environment/NYC-Building-Energy-and-Water-Data-Disclosure-for-/5zyy-y8am/about_data). \"The 1-100 scale is set so that 1 represents the worst performing buildings and 100 represents the best performing buildings. A score of 50 indicates that a building is performing at the national median, taking into account its size, location, and operating parameters. A score of 75 indicates that a property is performing in the 75th percentile and may be eligible to earn ENERGY STAR Certification. ENERGY STAR Scores are available for many property types. The 1-100 scale is based on the country in which your property is located. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ENERGY STAR Score'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a7521",
   "metadata": {},
   "source": [
    "# Address missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    # total missing values per column\n",
    "    total_missing = df.isnull().sum()\n",
    "\n",
    "    # percentage of missing values\n",
    "    percent_missing = 100* (total_missing / len(df))\n",
    "\n",
    "    # missing value table\n",
    "    table = pd.concat([total_missing, percent_missing], axis=1)\n",
    "\n",
    "    # rename the columns\n",
    "    table = table.rename(\n",
    "        columns = {0: 'Missing Values', 1: '% of Total'}\n",
    "    )\n",
    "\n",
    "    # sort by percentage, descending\n",
    "    table = table.sort_values(by='% of Total', ascending=False)\n",
    "\n",
    "    # print summmary information\n",
    "    print(f\"Dataframe has {df.shape[1]} columns.\")\n",
    "    print(f\"{len(table[table['% of Total']!=0])} columns have missing information.\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_table = missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "missing_data_table['% of Total'].hist()\n",
    "plt.xlabel('% of Total Missing')\n",
    "plt.ylabel('Number of Columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18597faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out columns that have >50% missing data\n",
    "\n",
    "columns_to_drop = list(missing_data_table[ missing_data_table['% of Total'] > 50 ].index)\n",
    "\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "print(\"Dataframe shape after dropping columns:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d62eb7",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "The next portion of the script will focus on Exploratory Data Analysis (EDA), which refers to the process of making plots and calculations on the data to learn more about its constituent features and data patterns. The purpose of EDA is to find anomalies, trends, or relationships which may be useful when informing modeling decisions downstream. EDA typically begins with a high-level overview and gradually narrows in on more specific features of the dataset. \n",
    "\n",
    "We'll start EDA by exploring the Energy Star Score (hereon refered to as just 'score'), which is the target variable for our machine learning model. We'll start with a simple distribution of scores in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf72784",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(data['ENERGY STAR Score'].dropna(),bins=100, edgecolor='k')\n",
    "plt.xlabel(\"ENERGY STAR Score\")\n",
    "plt.xticks(np.arange(0, 101, 20),np.arange(0, 101, 20));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3674b",
   "metadata": {},
   "source": [
    "It's peculiar that there are strong peaks at scores of 0 and 100. We would expect this distribution to e fairly flat given that energy star scores are equivalent to percentile ranks, according to the official definition (linked [here](https://data.cityofnewyork.us/Environment/NYC-Building-Energy-and-Water-Data-Disclosure-for-/5zyy-y8am/about_data)). The score is however self-reported and thus the anomaly at 100 could indicate that some property owners are over-estimating their star score rating. The peak at 0 is perhaps due to default values set when a self-reported value doesn't exist. Though the score is self-reported, it relies on a number of factors which are rigorously measured and, thus, more reliable as a metric of energy efficiency. One of these factors is the Site EUI (energy usage intensity), which measures energy usage per square foot. We'll explore this column by first observing summary statistics, followed by the distribution of values that exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary stats:\\n\", data['Site EUI (kBtu/ftÂ²)'].describe())\n",
    "print(\"\\nLargest values:\\n\", data['Site EUI (kBtu/ftÂ²)'].dropna().sort_values().tail(10))\n",
    "\n",
    "data.loc[data['Site EUI (kBtu/ftÂ²)']==869265, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb509c16",
   "metadata": {},
   "source": [
    "Upon the first iteration of plotting Site EUI, I produced a histogram that was uninterpretable due to an extreme outlier. To investigate the outlier, I printed the summary stats above and printed the largest values in the 'Site EUI' column. I found one row with the value `869,265.0` while all others were `< 150,000`. Since there was only one outlier in a database with nearly 12,000 rows, I decided to drop this outlier. This was done using the IQR method in the next cell, followed by the final distribution of Site EUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier with IQR\n",
    "first_quartile = data['Site EUI (kBtu/ftÂ²)'].quantile(0.25)\n",
    "third_quartile = data['Site EUI (kBtu/ftÂ²)'].quantile(0.75)\n",
    "\n",
    "# IQR (interquartile range)\n",
    "iqr = third_quartile - first_quartile\n",
    "\n",
    "# outliers are either below (0.25 - 3IQR) or above (0.75 + 3IQR)\n",
    "data = data[\n",
    "    (data['Site EUI (kBtu/ftÂ²)'] > (first_quartile - 3 * iqr)) & \n",
    "    (data['Site EUI (kBtu/ftÂ²)'] < (third_quartile + 3 * iqr))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f68d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "n_bins = 100\n",
    "min = data['Site EUI (kBtu/ftÂ²)'].min()\n",
    "max = data['Site EUI (kBtu/ftÂ²)'].quantile(0.75)\n",
    "width = (max-min)/n_bins\n",
    "\n",
    "bins = np.arange(min, max, width)\n",
    "\n",
    "plt.hist(data['Site EUI (kBtu/ftÂ²)'])\n",
    "plt.title('Site EUI (kBtu/ftÂ²)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07223166",
   "metadata": {},
   "source": [
    "This distribution appears Gaussian with a positive skew (long tail on the right). The sharp peaks observed in the score distribution do not reappear in this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore unique values in the column that indicates property type\n",
    "column_name = 'Largest Property Use Type'\n",
    "property_types = list(data[column_name].unique())\n",
    "\n",
    "print(f\"\\n\\n{column_name}:\", data[column_name].unique())\n",
    "print(f\"\\nDataset contains {data[column_name].nunique()} unique property types from column `{column_name}`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f85859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore unique values in the column that indicates property type\n",
    "column_name = 'Primary Property Type - Self Selected'\n",
    "property_types = list(data[column_name].unique())\n",
    "\n",
    "print(f\"\\n\\n{column_name}:\", data[column_name].unique())\n",
    "print(f\"\\nDataset contains {data[column_name].nunique()} unique property types from column `{column_name}`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of Site EUI based on property type in 'largest use' column\n",
    "\n",
    "# set designated column for determining property type\n",
    "column_name = 'Largest Property Use Type'\n",
    "\n",
    "# drop rows with missing score values\n",
    "data_with_scores = data.dropna(subset=['ENERGY STAR Score'])\n",
    "property_counts = data_with_scores[column_name].value_counts()\n",
    "property_types = list(property_counts[property_counts.values > 100].index)\n",
    "\n",
    "# initiate new figure\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "# iterate through property types\n",
    "for each_type in property_types[0:5]:\n",
    "    # filter out relevant rows\n",
    "    subset = data[data[column_name] == each_type ]\n",
    "    \n",
    "    # add to density plot\n",
    "    sns.kdeplot(\n",
    "        subset['ENERGY STAR Score'].dropna(),\n",
    "        label = each_type,\n",
    "        fill = False,\n",
    "        alpha = 0.5\n",
    "        )\n",
    "plt.legend()\n",
    "plt.title(\"Energy Star Scores by Property Type (according to Largest Use)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of Site EUI based on property type in 'largest use' column\n",
    "\n",
    "# set designated column for determining property type\n",
    "column_name = 'Primary Property Type - Self Selected'\n",
    "\n",
    "# drop rows with missing score values\n",
    "data_with_scores = data.dropna(subset=['ENERGY STAR Score'])\n",
    "property_counts = data_with_scores[column_name].value_counts()\n",
    "property_types = list(property_counts[property_counts.values > 100].index)\n",
    "\n",
    "# initiate new figure\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "# iterate through property types\n",
    "for each_type in property_types[0:5]:\n",
    "    # filter out relevant rows\n",
    "    subset = data[data[column_name] == each_type ]\n",
    "    \n",
    "    # add to density plot\n",
    "    sns.kdeplot(\n",
    "        subset['ENERGY STAR Score'].dropna(),\n",
    "        label = each_type,\n",
    "        fill = False,\n",
    "        alpha = 0.5\n",
    "        )\n",
    "plt.legend()\n",
    "plt.title(\"Energy Star Scores by Property Type (according to Self-Selected Type)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789cdff",
   "metadata": {},
   "source": [
    "The figures above agree well with each other, which is a good sanity check to see that the two categorical columns indicating property type agree well with one another. Since property type does have a role in the star score distribution, we should feed it into the machine learning model (after one-hot encoding). We'll elect to use the category based on 'Largest Use' rather than the self-reported property type, since it is a more quantitative measure.\n",
    "\n",
    "In addition to property types, there are other categorical columns such as the borough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of Site EUI based on borough\n",
    "\n",
    "# set designated column for determining property type\n",
    "column_name = 'Borough'\n",
    "\n",
    "# drop rows with missing score values\n",
    "data_with_scores = data.dropna(subset=['ENERGY STAR Score'])\n",
    "borough_counts = data_with_scores[column_name].value_counts()\n",
    "borough_types = list(borough_counts[borough_counts.values > 100].index)\n",
    "\n",
    "# initiate new figure\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "# iterate through property types\n",
    "for each_type in borough_types[0:5]:\n",
    "    # filter out relevant rows\n",
    "    subset = data[data[column_name] == each_type ]\n",
    "    \n",
    "    # add to density plot\n",
    "    sns.kdeplot(\n",
    "        subset['ENERGY STAR Score'].dropna(),\n",
    "        label = each_type,\n",
    "        fill = False,\n",
    "        alpha = 0.5\n",
    "        )\n",
    "plt.legend()\n",
    "plt.title(\"Energy Star Scores by Property Type (according to Self-Selected Type)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd76dc",
   "metadata": {},
   "source": [
    "The borough of the building does not seem to make as significant a difference in the distribution of the score as does the building type. Nonetheless, it might make sense to include the borough as a categorical variable given small fluctuations that differ between borough density plots. \n",
    "\n",
    "One final density plot we can make is to break out buildings by year of construction. This column is already numeric and not categorical, but we can nonetheless extend the plots from above to a new density plot based on 'Year Built.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380315a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of Site EUI based on year build\n",
    "\n",
    "# set designated column for determining property type\n",
    "column_name = 'Year Built'\n",
    "\n",
    "# drop rows with missing score values\n",
    "data_with_scores = data.dropna(subset=['ENERGY STAR Score'])\n",
    "cat_counts = data_with_scores[column_name].value_counts()\n",
    "cat_types = list(cat_counts[cat_counts.values > 100].index)\n",
    "cat_types.sort()\n",
    "\n",
    "# initiate new figure\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "# iterate through property types\n",
    "for each_type in (cat_types[0:3] +  cat_types[-3:]):\n",
    "    # filter out relevant rows\n",
    "    subset = data[data[column_name] == each_type ]\n",
    "    \n",
    "    # add to density plot\n",
    "    sns.kdeplot(\n",
    "        subset['ENERGY STAR Score'].dropna(),\n",
    "        label = each_type,\n",
    "        fill = False,\n",
    "        alpha = 0.5,\n",
    "        )\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Energy Star Scores by Property Type (according to Self-Selected Type)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99532fb0",
   "metadata": {},
   "source": [
    "Above I have filtered out a list of years during which more than 100 properties were constructed. From that list I singled out the oldest 3 years and most recent 3 years, which make up the 6 lines in the density plot. We see that more recent years tend to have a higher density of properties with higher energy star scores. This is great! It indicates improved energy efficiency over time. I'm curious to see if this is at all related not only to year of construction, but property type. For example, the previous plots showed that office buildings tend to rank better in terms of energy scores. If most of the buildings put up recently are office buildings, then this may indicate that the higher scores from 2020-2022 are due to property types and not just improved construction/engineering practices.\n",
    "\n",
    "To investigate this issue, we can make stacked bar plots to see the distribution of property types constructed each year. We'll focus on the time range 1965 to 2025, iterating through 20-year windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf459b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ranges = [ \n",
    "    [1965, 1985],\n",
    "    [1985, 2005],\n",
    "    [2005, 2025]\n",
    "]\n",
    "\n",
    "for each_range in year_ranges:\n",
    "    # Step 1: Get top 5 most common property use types\n",
    "    top5_types = data['Largest Property Use Type'].value_counts().nlargest(5).index\n",
    "\n",
    "    # Step 2: Filter to only those 5 types\n",
    "    filtered_data = data[\n",
    "        (data['Largest Property Use Type'].isin(top5_types)) &\n",
    "        (data['Year Built'].between(each_range[0], each_range[1]))\n",
    "    ]\n",
    "\n",
    "    # Step 3: Create pivot table: rows=Year Built, columns=Property Type, values=counts\n",
    "    pivot_table = (\n",
    "        filtered_data\n",
    "        .groupby(['Year Built', 'Largest Property Use Type'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # Step 4: Plot stacked bar chart\n",
    "    pivot_table.plot(kind='bar', stacked=True, figsize=(15, 6), colormap='tab10')\n",
    "\n",
    "    plt.title(\"Number of Buildings by Year (Top 5 Property Use Types)\")\n",
    "    plt.xlabel(\"Year Built\")\n",
    "    plt.ylabel(\"Number of Buildings\")\n",
    "    plt.legend(title='Property Use Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416a194",
   "metadata": {},
   "source": [
    "Here are some summary bullet points based on trends in the stacked bar plots above:\n",
    "\n",
    "- Construction of buildings was relatively constant from 1966 to 1985, with the exception of increased buildings in 1965, 1970, and 1985.\n",
    "- The majority of new bulidings that are put up every year are multi-family housing.\n",
    "- A higher fraction of new buildings were office bulidings in 1970 and 1986.\n",
    "- There was a steady decline in construction rates from 1986 to 1997 (pertaining only to those properties which are required to report energy usage for L84).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3c61e",
   "metadata": {},
   "source": [
    "# Correlation Studies\n",
    "\n",
    "We can search for correlations between features using the Pearson correlation coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number]).dropna()\n",
    "correlation_matrix = numeric_data.corr()\n",
    "correlation_series = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)).stack()\n",
    "\n",
    "\n",
    "top_correlations = correlation_series.nlargest(3)\n",
    "top_features = correlation_series.abs().sort_values(ascending=False).head(3).index.tolist()\n",
    "\n",
    "columns_to_keep = []\n",
    "for each_pair in top_features:\n",
    "    if each_pair[0] not in columns_to_keep:\n",
    "        columns_to_keep.append(each_pair[0])\n",
    "    if each_pair[1] not in columns_to_keep:\n",
    "        columns_to_keep.append(each_pair[1])\n",
    "\n",
    "print((columns_to_keep))\n",
    "high_corr_data = data[columns_to_keep]\n",
    "sns.heatmap(high_corr_data.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c72c2",
   "metadata": {},
   "source": [
    "The correlation matrix above allows shows that many columns have a nearly-equivalent partner column. Moreover, there are two versions of many of the figures, one for `Source` and another for `Site`. For example, EUI is split into `Source EUI` and `Site EUI`. Based on this [document](https://www.nyc.gov/html/gbee/downloads/pdf/nyc_ll84_benchmarking_report_2012.pdf), source energy usage figures refer to the total energy needed to create the consumption levels on-site. Site energy usage does not take into account energy generation or losses and is thus a less comprehensive estimate of a property's total energy cost. In addition to site vs source energy usage, the correlation matrix shows us that energy usage columns are also partnered with a weather-normalized column, which exhibit nearly identical correlation coefficients. This is why the 6x6 matrix apears more like a 3x3.\n",
    "\n",
    "Now let's investigate the correlation between numeric columns and the energy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be560f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_data = numeric_data.corr()['ENERGY STAR Score'].sort_values()\n",
    "print(correlations_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cad1e8",
   "metadata": {},
   "source": [
    "The strongest correlations occur between site EUI and source EUI. These correlations are negative, meaning that a higher EUI value is associated with a lower energy score. This makes sense, as a higher EUI is equivalent to a higher energy consumption per square footage. As we discovered in the previous correlation plot, site vs. source figures have similar correlation coefficients, suggesting that there is a linear relationship between site energy consumption and source energy consumption. We can pick only one of the two measurement standards for analysis.\n",
    "\n",
    "To account for possible non-linear relationships, we can take square root and natural log transformations of the features and then calculate the correlation coefficients with the score. These transformations would transform exponential or quadratic relationships to a more linear form. To try and capture any possible relationships between the borough or building type (remember these are categorical variables) and the score we will have to one-hot encode these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numeric columns\n",
    "numeric_subset = data.select_dtypes('number')\n",
    "\n",
    "# Create columns with square root and log of numeric columns\n",
    "for col in numeric_subset.columns:\n",
    "    # Skip the Energy Star Score column\n",
    "    if col == 'ENERGY STAR Score':\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])\n",
    "        numeric_subset['log_' + col] = np.log(numeric_subset[col])\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_subset = data[['Borough', 'Largest Property Use Type']]\n",
    "\n",
    "# One hot encode\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Drop buildings without an energy star score\n",
    "features = features.dropna(subset = ['ENERGY STAR Score'])\n",
    "\n",
    "# Find correlations with the score \n",
    "correlations = features.corr()['ENERGY STAR Score'].dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c195a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most negative correlations\n",
    "correlations.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most positive correlations\n",
    "correlations.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08882954",
   "metadata": {},
   "source": [
    "Even after data transformations, Site UI is still the most strong negatively-correlated feature to energy scores. There are no strong positive linear relationships although we do see that a building type of office (`Largest Property Use Type_Office`) is slightly positively correlated with the score. This variable is a one-hot encoded representation of the categorical variables for building type. To visualize the correlation between energy scores and site EUI, we can make a scatter plot. We'll use different colors to distinguish different property types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde094f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "column_name = 'Largest Property Use Type'\n",
    "property_types = list(data[column_name].unique()[0:5])\n",
    "\n",
    "# Set figure size\n",
    "figsize(12, 10)\n",
    "\n",
    "# Extract building types for rows where 'score' is not NaN\n",
    "features['Largest Property Use Type'] = data.dropna(subset=['ENERGY STAR Score'])['Largest Property Use Type']\n",
    "\n",
    "# Filter to building types with more than 100 observations\n",
    "features = features[features['Largest Property Use Type'].isin(property_types)]\n",
    "\n",
    "# Create the scatterplot using seaborn\n",
    "sns.lmplot(\n",
    "    data=features,\n",
    "    x='Site EUI (kBtu/ftÂ²)',\n",
    "    y='ENERGY STAR Score',\n",
    "    hue='Largest Property Use Type',\n",
    "    scatter_kws={'alpha': 0.8, 's': 60},\n",
    "    fit_reg=False,\n",
    "    height=12,\n",
    "    aspect=1.2\n",
    ")\n",
    "\n",
    "# Customize labels and title\n",
    "plt.xlabel(\"Site EUI\", fontsize=28)\n",
    "plt.ylabel(\"Energy Star Score\", fontsize=28)\n",
    "plt.title(\"Energy Star Score vs Site EUI\", fontsize=36)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000411",
   "metadata": {},
   "source": [
    "This plot confirms the relationship we saw in the correlation coefficient: the energy score is inversely related to the site EUI. Though the relationship doesn't look linear, the site EUI will be important for predicting energy scores in unseen data.\n",
    "\n",
    "As a final exercise for exploratory data analysis, we can make a pairs plot between several different variables. Pairs plots are scatter plots between pairs of variables and histograms of single variables on the diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns to plot\n",
    "plot_data = features[['ENERGY STAR Score', 'Site EUI (kBtu/ftÂ²)', \n",
    "                      'Weather Normalized Source EUI (kBtu/ftÂ²)', \n",
    "                      'log_Total (Location-Based) GHG Emissions (Metric Tons CO2e)']]\n",
    "\n",
    "# Replace inf with nan\n",
    "plot_data = plot_data.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Rename columns for cleaner plot labels\n",
    "plot_data = plot_data.rename(columns={\n",
    "    'Site EUI (kBtu/ftÂ²)': 'Site EUI',\n",
    "    'Weather Normalized Source EUI (kBtu/ftÂ²)': 'Weather Norm EUI',\n",
    "    'log_Total (Location-Based) GHG Emissions (Metric Tons CO2e)': 'log GHG Emissions'\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "plot_data = plot_data.dropna()\n",
    "\n",
    "# Function to annotate correlation\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(f\"r = {r:.2f}\", xy=(.2, .8), xycoords=ax.transAxes, size=20)\n",
    "\n",
    "# Create the PairGrid\n",
    "grid = sns.PairGrid(data=plot_data, height=3)\n",
    "\n",
    "# Map plots to upper, diagonal, and lower\n",
    "grid.map_upper(plt.scatter, color='red', alpha=0.6)\n",
    "grid.map_diag(plt.hist, color='red', edgecolor='black')\n",
    "grid.map_lower(corr_func)\n",
    "grid.map_lower(sns.kdeplot, cmap='Reds')\n",
    "\n",
    "# Add overall title\n",
    "plt.suptitle('Pairs Plot of Energy Data', size=36, y=1.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1acce",
   "metadata": {},
   "source": [
    "The energy score also exhibits a strong correlation with weather-normalized source EUI. The log GHG emissions is the only figure here with a flat density plot (lower-left corner). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c1f3b",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection\n",
    "\n",
    "Now we can move on to engineering a set of features for use in machine learning models based on the results of the EDA. In particular, we learned the following from EDA which can help us in engineering/selecting features:\n",
    "\n",
    "* The score distribution varies by building type and to a lesser extent by borough. Although we will focus on numerical features, we should also include these two categorical features in the model. \n",
    "* Taking the log transformation of features does not result in significant increases in the linear correlations between features and the score\n",
    "\n",
    "\n",
    "Before we get any further, we should define what feature engineering and selection are! These definitions are informal and have considerable overlap, but I like to think of them as two separate processes:\n",
    "\n",
    "* __[Feature Engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)__: The process of taking raw data and extracting or creating new features that allow a machine learning model to learn a mapping beween these features and the target. This might mean taking transformations of variables, such as we did with the log and square root, or one-hot encoding categorical variables so they can be used in a model. Generally, I think of feature engineering as __adding__ additional features derived from the raw data.\n",
    "* __[Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)__: The process of choosing the most relevant features in your data. \"Most relevant\" can depend on many factors, but it might be something as simple as the highest correlation with the target, or the features with the [most variance](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html). In feature selection, we remove features that do not help our model learn the relationship between features and the target. This can help the model generalize better to new data and results in a more interpretable model. Generally, I think of feature selection as __subtracting__ features so we are left with only those that are most important.\n",
    "\n",
    "Feature engineering and selection are iterative processes that will usually require several attempts to get right. Often we will use the results of modeling, such as the feature importances from a random forest, to go back and redo feature selection, or we might later discover relationships that necessitate creating new variables. Moreover, these processes usually incorporate a mixture of domain knowledge and statistical qualitites of the data.\n",
    "\n",
    "[Feature engineering and selection](https://www.featurelabs.com/blog/secret-to-data-science-success/) often has the highest returns on time invested in a machine learning problem. It can take quite a while to get right, but is often more important than the exact algorithm and hyperparameters used for the model. If we don't feed the model the correct data, then we are setting it up to fail and we should not expect it to learn! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3237f4a",
   "metadata": {},
   "source": [
    "In this project, we will take the following steps for feature engineering:\n",
    "\n",
    "* Select only the numerical variables and two categorical variables (borough and property use type)\n",
    "* Add in the log transformation of the numerical variables\n",
    "* One-hot encode the categorical variables\n",
    "\n",
    "For feature selection, we will do the following:\n",
    "\n",
    "* Remove [collinear features](https://statinfer.com/204-1-9-issue-of-multicollinearity-in-python/)\n",
    "\n",
    "We will discuss collinearity (also called [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)) when we get to that process! \n",
    "\n",
    "The following code selects the numeric features, adds in log transformations of all the numeric features, selects and one-hot encodes the categorical features, and joins the sets of features together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data\n",
    "features = data.copy()\n",
    "for each in features.columns:\n",
    "    print(each)\n",
    "print(features.columns)\n",
    "\n",
    "units = ['ft', 'kBtu', 'Number of', 'Percent', 'Hours', 'gal', 'Tons', 'kWh', 'ENERGY STAR Score', 'kgal']\n",
    "\n",
    "for col in features.columns:\n",
    "    # check if there are any units or any other indication that this column is numeric\n",
    "    for token in units:\n",
    "        if token in col:\n",
    "            try:\n",
    "                features[col] = features[col].astype(float)\n",
    "            except:\n",
    "                print(f\"Couldn't convert column '{col}'.\")\n",
    "\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "print(numeric_cols)\n",
    "numeric_subset = data[numeric_cols]\n",
    "#numeric_subset = data.select_dtypes('number')\n",
    "\n",
    "# Create columns with log of numeric columns\n",
    "for col in numeric_subset.columns:\n",
    "    # Skip the Energy Star Score column\n",
    "    if col == 'score':\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['log_' + col] = np.log(numeric_subset[col])\n",
    "        \n",
    "# Select the categorical columns\n",
    "categorical_subset = data[['Borough', 'Largest Property Use Type']]\n",
    "\n",
    "# One hot encode\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e57f9",
   "metadata": {},
   "source": [
    "At this point, we have 11319 observations (buildings) with 103 different features (one column is the score). Not all of these features are likely to be important for predicting the score, and several of these features are also redundant because they are highly correlated. We will deal with this second issue below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0714971",
   "metadata": {},
   "source": [
    "## Remove Collinear Features\n",
    "\n",
    "Highly [collinear features](http://psychologicalstatistics.blogspot.com/2013/11/multicollinearity-and-collinearity-in.html) have a significant correlation coefficent between them. For example, in our dataset, the `Site EUI` and `Weather Norm EUI` are highly correlated because they are just slightly different means of calculating the energy use intensity. We touched upon this earlier when making correlation coefficients, but we can observe the collinearity more directly with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = data[['Weather Normalized Site EUI (kBtu/ftÂ²)', 'Site EUI (kBtu/ftÂ²)']].dropna()\n",
    "\n",
    "plt.plot(plot_data['Site EUI (kBtu/ftÂ²)'], plot_data['Weather Normalized Site EUI (kBtu/ftÂ²)'], 'bo')\n",
    "plt.xlabel('Site EUI'); plt.ylabel('Weather Norm EUI')\n",
    "plt.title('Weather Norm EUI vs Site EUI, R = %0.4f' % np.corrcoef(data[['Weather Normalized Site EUI (kBtu/ftÂ²)', 'Site EUI (kBtu/ftÂ²)']].dropna(), rowvar=False)[0][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ae6aa",
   "metadata": {},
   "source": [
    "While variables in a dataset are usually correlated to a small degree, highly collinear variables can be redundant in the sense that we only need to retain one of the features to give our model the necessary information.\n",
    "\n",
    "Removing collinear features is a method to reduce model complexity by decreasing the number of features and can help to increase model generalization.  It can also help us to interpret the model because we only have to worry about a single variable, such as EUI, rather than how both EUI and weather normalized EUI affect the score. \n",
    "\n",
    "There are a number of methods for removing collinear features, such as using the [Variance Inflation Factor](http://www.statisticshowto.com/variance-inflation-factor/). We will use a simpler metric, and remove features that have a correlation coefficient above a certain threshold with each other (not with the score because we want variables that are highly correlated with the score!) For a more thorough discussion of removing collinear variables, check out [this notebook on Kaggle](https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity/code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccf045",
   "metadata": {},
   "source": [
    "The following code removes the collinear features based on a threshold we select for the correlation coefficients by removing one of the two features that are compared. It also prints the correlations that it removes so we can see the effect of adjusting the threshold. We will use a threshold of 0.6 which removes one of a pair of features if the correlation coefficient between the features exceeds this value. \n",
    "\n",
    "Again, I did not actually write this code from scratch, but rather adapted it from a [Stack Overflow answer](https://stackoverflow.com/a/43104383)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1949248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        x: pandas dataframe\n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between Energy Star Score\n",
    "    y = x['ENERGY STAR Score']\n",
    "    x = x.drop(columns = ['ENERGY STAR Score'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    # Manually drop other correlated columns\n",
    "    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu/ftÂ²)', \n",
    "                          'Largest Property Use Type - Gross Floor Area (ftÂ²)'])\n",
    "    # Add the score back in to the data\n",
    "    x['ENERGY STAR Score'] = y\n",
    "               \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6317da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the collinear features above a specified correlation coefficient\n",
    "features = remove_collinear_features(features, 0.6);\n",
    "\n",
    "# Remove any columns with all na values\n",
    "features  = features.dropna(axis=1, how = 'all')\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473da24f",
   "metadata": {},
   "source": [
    "Our final dataset now has 64 features (one of the columns is the target). This is still quite a few, but mostly it is because we have one-hot encoded the categorical variables. Moreover, while a large number of features may be problematic for models such as linear regression, models such as the random forest perform implicit feature selection and automatically determine which features are important during training. There are other feature selection steps to take, but for now we will keep all the features we have and see how the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883afd6",
   "metadata": {},
   "source": [
    "#### Additional Feature Selection\n",
    "\n",
    "There are plenty of more methods for [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html). Some popular methods include [principal components analysis (PCA)](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) which transforms the features into a reduced number of dimensions that preserve the greatest variance, or [independent components analysis (ICA)](http://cs229.stanford.edu/notes/cs229-notes11.pdf) which aims to find the independent sources in a set of features. However, while these methods are effective at reducing the number of features, they create new features that have no physical meaning and hence make interpreting a model nearly impossible. \n",
    "\n",
    "These methods are very helpful for dealing with high-dimensional data and I would suggest [reading more on the topic](https://machinelearningmastery.com/feature-selection-machine-learning-python/) if you plan on dealing with machine learning problems! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d396416",
   "metadata": {},
   "source": [
    "## Split Into Training and Testing Sets\n",
    "\n",
    "In machine learning, we always need to separate our features into two sets:\n",
    "\n",
    "1. __Training set__ which we provide to our model during training along with the answers so it can learn a mapping between the features and the target. \n",
    "2. __Testing set__ which we use to evaluate the mapping learned by the model. The model has never seen the answers on the testing set, but instead, must make predictions using only the features. As we know the true answers for the test set, we can then compare the test predictions to the true test targets to ghet an estimate of how well our model will perform when deployed in the real world. \n",
    "\n",
    "For our problem, we will first extract all the buildings without an Energy Star Score (we don't know the true answer for these buildings so they will not be helpful for training or testing). Then, we will split the buildings with an Energy Star Score into a testing set of 30% of the buildings, and a training set of 70% of the buildings. \n",
    "\n",
    "Splitting the data into a random training and testing set is simple using scikit-learn. We can set the random state of the split to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the buildings with no score and the buildings with a score\n",
    "no_score = features[features['ENERGY STAR Score'].isna()]\n",
    "score = features[features['ENERGY STAR Score'].notnull()]\n",
    "\n",
    "print(no_score.shape)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758338d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the features and targets\n",
    "features = score.drop(columns='ENERGY STAR Score')\n",
    "targets = pd.DataFrame(score['ENERGY STAR Score'])\n",
    "\n",
    "# Replace the inf and -inf with nan (required for later imputation)\n",
    "features = features.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Split into 70% training and 30% testing set\n",
    "X, X_test, y, y_test = train_test_split(features, targets, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427165b6",
   "metadata": {},
   "source": [
    "We have 9815 properties with no score, 33905 buildings with a score in the training set, and 14531 buildings with a score in the testing set. We have one final step to take in this notebook: determining a naive baseline for our models to beat! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31766b70",
   "metadata": {},
   "source": [
    "# Establish a Baseline\n",
    "\n",
    "It's important to establish a naive baseline before we beginning making machine learning models. If the models we build cannot outperform a naive guess then we might have to admit that machine learning is not suited for this problem. This could be because we are not using the right models, because we need more data, or because there is a simpler solution that does not require machine learning. Establishing a baseline is crucial so we do not end up building a machine learning model only to realize we can't actually solve the problem.\n",
    "\n",
    "For a regression task, a good naive baseline is to predict the median value of the target on the training set for all examples on the test set. This is simple to implement and sets a relatively low bar for our models: if they cannot do better than guessing the medin value, then we will need to rethink our approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b6349",
   "metadata": {},
   "source": [
    "## Metric: Mean Absolute Error\n",
    "\n",
    "There are a number of metrics used in machine learning tasks and it can be [difficult to know which one to choose](https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/). Most of the time it will depend on the particular problem and if you have a specific goal to optimize for. I like [Andrew Ng's advice to use a single real-value performance metric](https://www.coursera.org/learn/machine-learning-projects/lecture/wIKkC/single-number-evaluation-metric) in order to compare models because it simplifies the evaluate process. Rather than calculating multiple metrics and trying to determine how important each one is, we should use a single number. In this case, because we doing regression, the [__mean absolute error__](https://people.duke.edu/~rnau/compare.htm) is an appropriate metric. This is also interpretable because it represents the average amount our estimate if off by in the same units as the target value. \n",
    "\n",
    "The function below calculates the mean absolute error between true values and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74eee75",
   "metadata": {},
   "source": [
    "Now we can make the median guess and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_guess = np.median(y)\n",
    "\n",
    "print('The baseline guess is a score of %0.2f' % baseline_guess)\n",
    "print(\"Baseline Performance on the test set: MAE = %0.4f\" % mae(y_test, baseline_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103b2b1",
   "metadata": {},
   "source": [
    "This shows our average estimate on the test set is off by about 25 points. The scores are between 1 and 100 so this means the average error from a naive method if about 25%. The naive method of guessing the median training value provides us a low baseline for our models to beat! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbffcc9",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we carried out the first three [steps of a machine learning](https://towardsdatascience.com/the-7-steps-of-machine-learning-2877d7e5548e?gi=30cd995093a9) problem:\n",
    "\n",
    "1. Cleaned and formatted the raw data \n",
    "2. Performed an exploratory data analysis\n",
    "3. Developed a set of features to train our model using feature engineering and feature selection\n",
    "\n",
    "We also completed the crucial task of establishing a baseline metric so we can determine if our model is better than guessing! \n",
    "\n",
    "In the next notebook, we will focus on implementing several machine learning methods, selecting the best model, and optimizing it for our problem using hyperparameter tuning with cross validation. As a final step here, we will save the datasets we developed to use again in the next part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3637160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Now save the files\n",
    "no_score.to_csv('../data/no_score.csv', index=False)\n",
    "X.to_csv('../data/training_features.csv', index=False)\n",
    "X_test.to_csv('../data/testing_features.csv', index=False)\n",
    "y.to_csv('../data/training_labels.csv', index=False)\n",
    "y_test.to_csv('../data/testing_labels.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
